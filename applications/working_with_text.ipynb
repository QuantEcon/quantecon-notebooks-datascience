{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text\n",
    "\n",
    "> **Author**\n",
    "- [Paul Schrimpf *UBC*](https://economics.ubc.ca/faculty-and-staff/paul-schrimpf/)\n",
    "\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- [Visualization Rules](visualization_rules.ipynb)  \n",
    "- [Regression](regression.ipynb)  \n",
    "- [Classification](classification.ipynb)  \n",
    "- [Maps](maps.ipynb)  \n",
    "\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Use text as features for classification  \n",
    "- Understand latent topic analysis  \n",
    "- Use folium to create an interactive map  \n",
    "- Request and combine json data from a webserver  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [Working with Text](#Working-with-Text)  \n",
    "  - [Introduction](#Introduction)  \n",
    "  - [Avalanches](#Avalanches)  \n",
    "  - [Predicting Incidents from Text](#Predicting-Incidents-from-Text)  \n",
    "  - [Unsupervised Learning](#Unsupervised-Learning)  \n",
    "  - [Exercises](#Exercises)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Uncomment following line to install on colab\n",
    "#! pip install qeds fiona geopandas xgboost gensim folium pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Many data sources contain text as well as numerical data.\n",
    "\n",
    "Text can be used to create features for any of the prediction methods\n",
    "that we have discussed.\n",
    "\n",
    "Doing so requires encoding text into some numerical representation.\n",
    "\n",
    "A good encoding preserves the meaning in the original text, while\n",
    "keeping dimensionality manageable.\n",
    "\n",
    "In this lecture, we will learn how to work with text through an\n",
    "application — predicting avalanche fatalities from avalanche\n",
    "forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avalanches\n",
    "\n",
    "Snow avalanches are a hazard in the mountains. Avalanches can be\n",
    "partially predicted based on snow conditions, weather, and\n",
    "terrain. [Avalanche Canada](https://www.avalanche.ca/map) produces\n",
    "daily avalanche forecasts for various mountainous regions in\n",
    "Canada. These forecasts consist of 1-5 ratings for each of three\n",
    "elevation bands, and textual descriptions of the recent avalanche\n",
    "observations, the snowpack, and weather. Avalanche Canada also\n",
    "maintains a list of [fatal avalanche incidents](https://www.avalanche.ca/incidents/) . In this lecture, we will\n",
    "attempt to predict fatal incidents from the text of avalanche\n",
    "forecasts. Since fatal incidents are rare, this prediction task will\n",
    "be quite difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# activate plot theme\n",
    "import qeds\n",
    "qeds.themes.mpl_style();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Avalanche Canada has an unstable json api. The api seems to be largely\n",
    "tailored to displaying the information on various Avalanche Canada\n",
    "websites and does not make it easy to obtain large amounts of\n",
    "data. Nonetheless, it is easier to get information from the API than\n",
    "to scrape the website. Generally, anytime you’re considering scraping\n",
    "a website, you should check whether the site has an API that you could\n",
    "use instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incident Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Get data on avalanche forecasts and incidents from Avalanche Canada\n",
    "# Avalanche Canada has an unstable public api\n",
    "# https://github.com/avalanche-canada/ac-web\n",
    "# Since API might change, this code might break\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Incidents\n",
    "url = \"http://incidents.avalanche.ca/public/incidents/?format=json\"\n",
    "req = urllib.request.Request(url)\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    result = json.loads(response.read().decode('utf-8'))\n",
    "incident_list = result[\"results\"]\n",
    "while (result[\"next\"] != None) :\n",
    "    req = urllib.request.Request(result[\"next\"])\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    incident_list = incident_list + result[\"results\"]\n",
    "incidents_brief = pd.DataFrame.from_dict(incident_list,orient=\"columns\")\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8\n",
    "incidents_brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# We can get more information about these incidents e.g. \"https://www.avalanche.ca/incidents/37d909e4-c6de-43f1-8416-57a34cd48255\"\n",
    "# this information is also available through the API\n",
    "def get_incident_details(id) :\n",
    "    url = \"http://incidents.avalanche.ca/public/incidents/{}?format=json\".format(id)\n",
    "    req = urllib.request.Request(url)\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    return(result)\n",
    "\n",
    "\n",
    "incidentsfile=\"avalanche_incidents.csv\"\n",
    "\n",
    "# To avoid loading the avalanche canada servers, we save the incident details locally\n",
    "if (not os.path.isfile(incidentsfile)) :\n",
    "    incident_detail_list = incidents_brief.id.apply(get_incident_details).to_list()\n",
    "    incidents = pd.DataFrame.from_dict(incident_detail_list,orient=\"columns\")\n",
    "    incidents.to_csv(incidentsfile)\n",
    "else :\n",
    "    incidents = pd.read_csv(incidentsfile)\n",
    "\n",
    "incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the incidents include coordinates, but others do not. Most\n",
    "however, include a place name. We can use [Natural Resources Canada’s\n",
    "Geolocation Service](https://www.nrcan.gc.ca/earth-sciences/geography/topographic-information/geolocalisation-service/17304)\n",
    "to retrieve coordinates from place names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# geocode locations without coordinates\n",
    "def geolocate(location,province) :\n",
    "    url = \"http://geogratis.gc.ca/services/geolocation/en/locate?q={},%20{}\".format(urllib.parse.quote(location),province)\n",
    "    req = urllib.request.Request(url)\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    if (len(result)==0) :\n",
    "        return([None,None])\n",
    "    else :\n",
    "        return(result[0]['geometry']['coordinates'])\n",
    "if not \"alt_coord\" in incidents.columns :\n",
    "    incidents[\"alt_coord\"] = [geolocate(incidents.location[i], incidents.location_province[i]) for i in incidents.index]\n",
    "    incidents.to_csv(incidentsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have incident data, let’s create some figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# clean up activity names\n",
    "incidents.group_activity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "incidents.group_activity=incidents.group_activity.replace(\"Ski touring\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Out-of-Bounds Skiing\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Lift Skiing Closed\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Skiing\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Snowshoeing\",\"Snowshoeing & Hiking\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Snowshoeing and Hiking\",\"Snowshoeing & Hiking\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Mechanized Skiing\",\"Heli or Cat Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Heliskiing\",\"Heli or Cat Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"At Outdoor Worksite\",\"Work\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Control Work\",\"Work\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Hunting/Fishing\",\"Other Recreational\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Inside Car/Truck on Road\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Car/Truck on Road\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Inside Building\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Outside Building\",\"Car/Truck/Building\")\n",
    "\n",
    "\n",
    "incidents.group_activity.unique()\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(12,4))\n",
    "colors=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "incidents.groupby(['group_activity']).id.count().plot(kind='bar', title=\"Incidents by Activity\", ax=ax[0])\n",
    "incidents.groupby(['group_activity']).num_fatal.sum().plot(kind='bar', title=\"Deaths by Activity\", ax=ax[1], color=colors[1])\n",
    "ax[0].set_xlabel(None)\n",
    "ax[1].set_xlabel(None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "incidents[\"date\"] = pd.to_datetime(incidents.ob_date)\n",
    "incidents[\"year\"] = incidents.date.apply(lambda x: x.year)\n",
    "incidents.date = incidents.date.dt.date\n",
    "colors=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "f = incidents.groupby([\"year\"]).num_fatal.sum()\n",
    "n = incidents.groupby([\"year\"]).id.count()\n",
    "yearstart=1950\n",
    "f=f[f.index>yearstart]\n",
    "n=n[n.index>yearstart]\n",
    "fig,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "n.plot(ax=ax)\n",
    "f.plot(ax=ax)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.annotate(\"Incidents\", (2010, 4), color=colors[0])\n",
    "ax.annotate(\"Deaths\", (2011, 15), color=colors[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Incidents\n",
    "\n",
    "Since the incident data includes coordinates, we might as well make a\n",
    "map too. Unfortunately, there are some obvious errors in some of the\n",
    "latitude and longitudes. Here, we try to fix the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# fix errors in latitude, longitude\n",
    "latlon = incidents.location_coords\n",
    "def makenumeric(cstr) :\n",
    "    if cstr==None :\n",
    "        return([None,None])\n",
    "    elif (type(cstr)==str):\n",
    "        return([float(s) for s in re.findall(r'-?\\d+\\.?\\d*',cstr)])\n",
    "    else :\n",
    "        return(cstr)\n",
    "\n",
    "latlon = latlon.apply(makenumeric)\n",
    "\n",
    "def good_lat(lat) :\n",
    "    return(lat >= 41.6 and lat <= 83.12) # min & max for Canada\n",
    "def good_lon(lon) :\n",
    "    return(lon >= -141 and lon<= -52.6)\n",
    "def fixlatlon(c) :\n",
    "    if (len(c)<2 or type(c[0])!=float or type(c[1])!=float) :\n",
    "        c = [None, None]\n",
    "        return(c)\n",
    "    lat = c[0]\n",
    "    lon = c[1]\n",
    "    if not good_lat(lat) and good_lat(lon) :\n",
    "        tmp = lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lon(lon) and good_lon(-lon) :\n",
    "        lon = -lon\n",
    "    if not good_lon(lon) and good_lon(lat) :\n",
    "        tmp = lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lon(lon) and good_lon(-lat) :\n",
    "        tmp = -lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lat(lat) or not good_lon(lon) :\n",
    "        c[0] = None\n",
    "        c[1] = None\n",
    "    else :\n",
    "        c[0] = lat\n",
    "        c[1] = lon\n",
    "    return(c)\n",
    "\n",
    "incidents[\"latlon\"] = latlon.apply(fixlatlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def foo(c,a) :\n",
    "    if (type(a)==str) :\n",
    "        a = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*',a)]\n",
    "    if len(a) <2 :\n",
    "        a = [None,None]\n",
    "    return([a[1],a[0]] if type(c[0])!=float else c)\n",
    "incidents[\"latlon_filled\"]=[foo(c,a) for c,a in zip(incidents[\"latlon\"],incidents[\"alt_coord\"])]\n",
    "nmiss = sum([a[0]==None for a in incidents.latlon_filled])\n",
    "n = len(incidents.latlon_filled)\n",
    "print(\"{} of {} incidents have latitude & longitude\".format(n-nmiss, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# download forecast region definitions\n",
    "req = urllib.request.Request(\"https://www.avalanche.ca/api/forecasts\")\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    forecastregions = json.loads(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to uncomment the second line below if  folium is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Map forecast regions and incidents\n",
    "#!pip install --user folium\n",
    "import folium\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Set1')\n",
    "fmap = folium.Map(location=[60, -98],\n",
    "                            zoom_start=3,\n",
    "                            tiles='Stamen Terrain')\n",
    "folium.GeoJson(forecastregions,\n",
    "               tooltip=folium.GeoJsonTooltip(fields=[\"name\"], aliases=[\"\"]),\n",
    "               highlight_function=lambda x: { 'weight': 10},\n",
    "              style_function=lambda x: {'weight':1}).add_to(fmap)\n",
    "activities = incidents.group_activity.unique()\n",
    "for i in incidents.index :\n",
    "    if not None==incidents.latlon_filled[i][0] and not None==incidents.latlon_filled[i][1] :\n",
    "        cindex=[j for j,x in enumerate(activities) if x==incidents.group_activity[i]][0]\n",
    "        txt = \"{}, {}<br>{} deaths\"\n",
    "        txt = txt.format(incidents.group_activity[i],\n",
    "                        incidents.ob_date[i],\n",
    "                        incidents.num_fatal[i]\n",
    "                        )\n",
    "        pop = folium.Popup(incidents.comment[i], parse_html=True, max_width=400)\n",
    "        folium.CircleMarker(incidents.latlon_filled[i],\n",
    "                      tooltip=txt,\n",
    "                      popup=pop,\n",
    "                      color=matplotlib.colors.to_hex(cmap(cindex)), fill=True, radius=5).add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to click around the map and read about some of the incidents.\n",
    "\n",
    "Compare presenting this information on a map with the list on [https://www.avalanche.ca/incidents/](https://www.avalanche.ca/incidents/) .\n",
    "Which do you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Incidents to Regions\n",
    "\n",
    "Later we will want to match incidents to forecasts, so let’s find the closest region to each incident.\n",
    "\n",
    "Note that distance here will be in units of latitude, longitude (or\n",
    "whatever coordinate system we use). At the equator, a distance of 1 is\n",
    "approximately 60 nautical miles.\n",
    "\n",
    "Since longitude lines get closer together farther from the equator,\n",
    "these distances will be understated the further North you go.\n",
    "\n",
    "I do not think this is too much of a problem for just finding the\n",
    "nearest region, but if we care about accurate distances, we should\n",
    "reproject latitude and longitude into a different coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Match incidents to nearest forecast regions\n",
    "from shapely.geometry import Point, Polygon, shape\n",
    "point = Point(incidents.latlon_filled[0][1],incidents.latlon_filled[0][0])\n",
    "def distances(latlon) :\n",
    "    point=Point(latlon[1],latlon[0])\n",
    "    df = pd.DataFrame.from_dict([{'id':feature['id'],\n",
    "                                  'distance':shape(feature['geometry']).distance(point)} for\n",
    "                                 feature in forecastregions['features']])\n",
    "    return(df)\n",
    "def foo(x) :\n",
    "    if (x[0]==None) :\n",
    "        return(None)\n",
    "    d = distances(x)\n",
    "    return(d.id[d.distance.idxmin()])\n",
    "incidents['nearest_region'] = incidents.latlon_filled.apply(foo)\n",
    "incidents['nearest_distance'] = incidents.latlon_filled.apply(lambda x : None if x[0]==None else distances(x).distance.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecast Data\n",
    "\n",
    "We now download all forecasts for all regions since November 2011 (roughly the earliest data available).\n",
    "\n",
    "We can only request one forecast at a time, so this takes many hours to finish.\n",
    "\n",
    "The forecasts are saved to disk and included in the github repository to avoid re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Functions for downloading forecasts from Avalanche Canada\n",
    "\n",
    "def get_forecast(date, region) :\n",
    "    url = \"https://www.avalanche.ca/api/bulletin-archive/{}/{}.json\".format(date.isoformat(),region)\n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            result = json.loads(response.read().decode('utf-8'))\n",
    "        return(result)\n",
    "    except:\n",
    "        return(None)\n",
    "\n",
    "def get_forecasts(start, end, region) :\n",
    "    day = start\n",
    "    forecasts = []\n",
    "    while(day<=end and day<end.today()) :\n",
    "        #print(\"working on {}, {}\".format(region,day))\n",
    "        forecasts = forecasts + [get_forecast(day, region)]\n",
    "        #print(\"sleeping\")\n",
    "        time.sleep(0.1) # to avoid too much load on Avalanche Canada servers\n",
    "        day = day + pd.Timedelta(1,\"D\")\n",
    "    return(forecasts)\n",
    "\n",
    "def get_season(year, region) :\n",
    "    start_month = 11\n",
    "    start_day = 20\n",
    "    last_month = 5\n",
    "    last_day = 1\n",
    "    if (not os.path.isdir(\"avalanche_forecasts\")):\n",
    "        os.mkdir(\"avalanche_forecasts\")\n",
    "    seasonfile = \"avalanche_forecasts/{}_{}-{}.json\".format(region, year, year+1)\n",
    "    if (not os.path.isfile(seasonfile)) :\n",
    "        startdate = pd.to_datetime(\"{}-{}-{} 12:00\".format(year, start_month, start_day))\n",
    "        lastdate = pd.to_datetime(\"{}-{}-{} 12:00\".format(year+1, last_month, last_day))\n",
    "        season = get_forecasts(startdate,lastdate,region)\n",
    "        with open(seasonfile, 'w') as outfile:\n",
    "            json.dump(season, outfile, ensure_ascii=False)\n",
    "    else :\n",
    "        with open(seasonfile) as json_data:\n",
    "            season = json.load(json_data)\n",
    "    return(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "forecastlist=[]\n",
    "\n",
    "for year in range(2011,2019):\n",
    "    print(\"working on {}\".format(year))\n",
    "    for region in [region[\"id\"] for region in forecastregions[\"features\"]] :\n",
    "        forecastlist = forecastlist + get_season(year, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# convert to dataframe and extract some variables\n",
    "forecasts = pd.DataFrame.from_dict([f for f in forecastlist if not f==None],orient=\"columns\")\n",
    "\n",
    "forecasts[\"danger_date\"] = forecasts.dangerRatings.apply(lambda r : r[0][\"date\"])\n",
    "forecasts[\"danger_date\"] = pd.to_datetime(forecasts.danger_date).dt.date\n",
    "forecasts[\"danger_alpine\"]=forecasts.dangerRatings.apply(lambda r : r[0][\"dangerRating\"][\"alp\"])\n",
    "forecasts[\"danger_treeline\"]=forecasts.dangerRatings.apply(lambda r : r[0][\"dangerRating\"][\"tln\"])\n",
    "forecasts[\"danger_belowtree\"]=forecasts.dangerRatings.apply(lambda r : r[0][\"dangerRating\"][\"btl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# merge incidents to forecasts\n",
    "adf = pd.merge(forecasts, incidents, how=\"left\",\n",
    "               left_on=[\"region\",\"danger_date\"],\n",
    "               right_on=[\"nearest_region\",\"date\"],\n",
    "              indicator=True)\n",
    "adf[\"incident\"] = adf._merge==\"both\"\n",
    "print(\"There were {} incidents matched with forecasts data. These occured on {}% of day-regions with forecasts\".format(adf.incident.sum(),adf.incident.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ratings=sorted(adf.danger_alpine.unique())\n",
    "ava_colors = [\"#52BA4A\", \"#FFF300\", \"#F79218\", \"#EF1C29\", \"#1A1A1A\", \"#BFBFBF\"]\n",
    "for x in [\"danger_alpine\", \"danger_treeline\", \"danger_belowtree\"] :\n",
    "    fig=sns.catplot(x=x, kind=\"count\",col=\"incident\", order=ratings, data=adf, sharey=False,\n",
    "                    palette=ava_colors, height=3, aspect=2)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    fig.fig.suptitle(x.replace(\"danger_\",\"\"))\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Incidents from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The first step when using text as data is to preprocess the text.\n",
    "\n",
    "In preprocessing we will :\n",
    "\n",
    "1. Remove unwanted punctuation and non-text characters  \n",
    "1. Tokenize: break down sentences into words  \n",
    "1. Remove “stopwords”: words that are so common they provide no information, like “a” and “the”  \n",
    "1. Lemmatize words : reduce words to their dictionary “lemma” e.g. “snowing” and “snowed” both become snow (verb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Remove stop words (the, a, is, etc)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords=stopwords.union(set(string.punctuation))\n",
    "# Lemmatize words e.g. snowed and snowing are both snow (verb)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "def text_prep(txt) :\n",
    "    soup = BeautifulSoup(txt)\n",
    "    [s.extract() for s in soup('style')] # remove css\n",
    "    txt=soup.text # remove html tags\n",
    "    tokens = [token for token in nltk.tokenize.word_tokenize(txt)]\n",
    "    tokens = [token for token in tokens if not token in stopwords]\n",
    "    #tokens = [token for token in tokens if not token ]\n",
    "    tokens = [wnl.lemmatize(token) for token in tokens]\n",
    "    if (len(tokens)==0) :\n",
    "        tokens = [\"EMPTYSTRING\"]\n",
    "    return(tokens)\n",
    "\n",
    "text_prep(forecasts.highlights[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s apply this to all the avalanche summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "text_data = [text_prep(txt) for txt in adf.avalancheSummary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make a bar plot of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "wf = nltk.FreqDist([word for doc in text_data for word in doc]).most_common(20)\n",
    "words = [x[0] for x in wf]\n",
    "cnt = [x[1] for x in wf]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.bar(range(len(words)), cnt);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title('Most common words in avalanche summaries');\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The simplest approach to convert a collection of processed text\n",
    "documents to a feature matrix is the “bag of words” approach. We view\n",
    "each document as a bag of words and our feature matrix consists of\n",
    "counts of how many time each word appears. It’s called a bag of words\n",
    "because we ignore the order of words within the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "text_data = [text_prep(txt) for txt in adf.avalancheSummary]\n",
    "y = adf.incident\n",
    "X = vectorizer.fit_transform([' '.join(doc) for doc in text_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complicated feature engineering is also possible. One extension\n",
    "is to consider counts of pairs or triples of consecutive words. These\n",
    "are called n-grams, and can be created by setting the `n_gram`\n",
    "argument for `CountVectorizer`. Another alternative is to adjust for\n",
    "the fact that common words will inherently have higher counts using\n",
    "term-frequency inverse-document-frequency (see below).\n",
    "\n",
    "Having created our feature matrix, we can now apply any classification\n",
    "method to predict incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "A type of classifier that is often used with text data is the Naive Bayes classifier.\n",
    "This classifier predicts incidents using Bayes rules\n",
    "\n",
    "$$\n",
    "P(incident | words) = \\frac{P(words|incident) P(incidents)}{P(words)}\n",
    "$$\n",
    "\n",
    "It is Naive in that it assumes words are independent of one another given an incident.\n",
    "\n",
    "$$\n",
    "P(words|incident) = \\prod_{w \\in words} P(w|incident)\n",
    "$$\n",
    "\n",
    "Although this assumption is very implausible for text, the Naive Bayes\n",
    "classifier can be computed extremely quickly, and it sometimes performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(Xtrain,ytrain)\n",
    "np.mean(classifier.predict(Xtest)==ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(ytest, classifier.predict(Xtest)))\n",
    "print(metrics.classification_report(ytest, classifier.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# print text with highest predicted probabilities\n",
    "phat=classifier.predict_proba(X)[:,1]\n",
    "def remove_html(txt) :\n",
    "    soup = BeautifulSoup(txt)\n",
    "    [s.extract() for s in soup('style')] # remove css\n",
    "    return(soup.text)\n",
    "docs = [remove_html(txt) for txt in adf.avalancheSummary]\n",
    "txt_high = [(_,x) for _, x in sorted(zip(phat,docs), key=lambda pair: pair[0],reverse=True)]\n",
    "txt_high[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# print text with lowest predicted probabilities\n",
    "txt_low = [(_,x) for _, x in sorted(zip(phat,docs), key=lambda pair: pair[0])]\n",
    "txt_low[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='exercise-0'></a>\n",
    "> See exercise 1 in the [*exercise list*](#exerciselist-0)\n",
    "\n",
    "\n",
    "Predicting deaths from forecast text is very difficult because deaths\n",
    "are so rare. A prediction exercise more likely to succeed would be to\n",
    "predict the avalanche rating from the forecast text. However,\n",
    "predicting the avalanche rating from the forecast text is a very\n",
    "artificial task, with little practical use.\n",
    "\n",
    "Another alternative would be to gather more data on non-fatal\n",
    "avalanches. Avalanche Canada also has user-submitted “Mountain\n",
    "Information Network” reports. These reports include observations of\n",
    "natural avalanches, and information on non-fatal avalanche\n",
    "incidents. Since the data is user-submitted, it is messy and more\n",
    "difficult to work with. Nonetheless, trying to work with it would be\n",
    "good practice and could lead to some insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "The regression and classification methods that we have seen so far are\n",
    "examples of supervised learning — there is an observed outcome that\n",
    "we are trying to predict. In unsupervised learning we do not have an\n",
    "observed outcome to predict. Instead, we try to find informative\n",
    "patterns in the data. Unsupervised learning can be particularly useful\n",
    "with text data. We will look at two related techniques for topic\n",
    "modeling. These techniques attempt to extract distinct topics from a\n",
    "collection of text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n",
    "Latent semantic analysis is used by some search engines for ranking\n",
    "the similarity among documents. Latent semantic analysis begins with a\n",
    "term document matrix, $ X $. The term document matrix is a number\n",
    "of documents by number of terms matrix where the i,jth entry is the\n",
    "measure of how often term j appears in document i. This could be the\n",
    "same bag of words feature matrix we constructed above, or it could be\n",
    "some other measure. For this example, we will use the term-frequency,\n",
    "inverse-document-frequency representation.\n",
    "\n",
    "$$\n",
    "x^{tfidf}_{ij} = \\frac{\\text{occurences of term j in document\n",
    "i}}{\\text{length of document i}} \\log \\left(\\frac{\\text{number of\n",
    "documents}}{\\text{number of documents containing term j}}\\right)\n",
    "$$\n",
    "\n",
    "Given a term document matrix, $ X $, latent semantic analysis\n",
    "computes a lower rank approximation to $ X $ through the singular\n",
    "value decomposition. This lower rank approximation can potentially be\n",
    "interpreted, or be used in place of $ X $ for other learning\n",
    "algorithms. In other contexts, similar decompositions are referred to\n",
    "as principal components analysis, and as factor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# LSA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)\n",
    "X = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have computed a rank 10 approximation to the tf-idf matrix. We\n",
    "can see how much of the variance of the original matrix our 10\n",
    "components reproduce. We can also look at how all terms in the\n",
    "document contribute to each of the 10 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(svd_model.explained_variance_ratio_)\n",
    "print(svd_model.explained_variance_ratio_.cumsum())\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "comp_label=[]\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    message = \"\"\n",
    "    for t in sorted_terms:\n",
    "        message = message + \"{:.2f} * {} + \".format(t[1],t[0])\n",
    "    print(message)\n",
    "    comp_label.append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can attempt to visualize the components.\n",
    "\n",
    "The LSA reduced the dimensionality of the representation of documents\n",
    "from thousands (of term-frequency inverse-document-frequency) to ten\n",
    "components. Ten is more manageable than thousands, but it is still too\n",
    "many dimensions to visualize effectively. t-SNE is a technique for\n",
    "reducing dimensionality further. t-SNE is a nonlinear transformation\n",
    "of the data from 10 dimensions to 2, which tries to preserve the\n",
    "original clustering of points in 10 dimensions. In other words, if a\n",
    "set of documents are clustered close together in the 10 dimensional\n",
    "LSA space, then they will also be close together in the 2 dimensional\n",
    "t-SNE representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lsa_topic_matrix = svd_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "nplot = 2000 # reduce the size of the data to speed computation and make the plot less cluttered\n",
    "lsa_topic_sample = lsa_topic_matrix[np.random.choice(lsa_topic_matrix.shape[0], nplot, replace=False)]\n",
    "tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=500,\n",
    "                      n_iter=1000, verbose=10, random_state=0, angle=0.75)\n",
    "tsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-SNE model takes our 10 dimensional LSA topics and creates a\n",
    "non-linear projection onto two dimensional space. It can be useful for\n",
    "visualizing high-dimensional data. One word of caution though is that\n",
    "the output of the t-SNE model can depend on the parameters of the\n",
    "algorithm. Failure to see clear clusters in the t-SNE visualization\n",
    "could mean either the original data was not clustered in higher\n",
    "dimensional space, or perhaps just the t-SNE algorithm parameters were\n",
    "chosen poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('Paired')\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
    "n_topics=len(svd_model.components_)\n",
    "lsa_keys = np.argmax(lsa_topic_sample, axis=1)\n",
    "ax[0].scatter(x=tsne_lsa_vectors[:,0],y=tsne_lsa_vectors[:,1], color=[cmap(i) for i in lsa_keys], alpha=0.8)\n",
    "bbox_props = dict(boxstyle=\"round4,pad=0.1\", lw=0.2, fc=\"white\")\n",
    "for i in range(n_topics) :\n",
    "    m = tsne_lsa_vectors[lsa_keys==i, :].mean(axis=0)\n",
    "    ax[0].text(m[0], m[1], str(i), ha=\"center\", va=\"center\",\n",
    "               size=15, color=cmap(i),\n",
    "               bbox=bbox_props)\n",
    "    ax[1].text(0,1-(i+1)*1/(n_topics+1),\"Topic \" + str(i) + \" : \"+ comp_label[i],ha=\"left\", va=\"center\", color=cmap(i))\n",
    "    ax[1].axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we see two things immediately. First, most documents\n",
    "are closest to topic 0. Second, most of the various topics are not\n",
    "well separated in this space.\n",
    "\n",
    "\n",
    "<a id='exercise-1'></a>\n",
    "> See exercise 2 in the [*exercise list*](#exerciselist-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Analysis\n",
    "\n",
    "Latent dirichlet analysis (LDA) produces similar output as latent semantic\n",
    "analysis, but LDA often produces nicer results. The statistical theory\n",
    "underlying LSA is built on continuous $ X $ features. LDA uses\n",
    "similar ideas, but takes into account that text is discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# LDA\n",
    "import gensim\n",
    "# gensim works with a list of lists of tokens\n",
    "text_data = [text_prep(txt) for txt in forecasts.avalancheSummary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# convert to bag of words\n",
    "dictionary = gensim.corpora.Dictionary(text_data)\n",
    "bow_data = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(bow_data, num_topics = 5, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "#!pip install --user pyldavis\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, bow_data, dictionary, sort = True)\n",
    "lda_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='exercise-2'></a>\n",
    "> See exercise 3 in the [*exercise list*](#exerciselist-0)\n",
    "\n",
    "\n",
    "\n",
    "<a id='exercise-3'></a>\n",
    "> See exercise 4 in the [*exercise list*](#exerciselist-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "\n",
    "<a id='exerciselist-0'></a>\n",
    "**Exercise 1**\n",
    "\n",
    "Use another classification method to predict incidents. Check whether\n",
    "your method outperforms the Naive Bayes classifier.\n",
    "\n",
    "([*back to text*](#exercise-0))\n",
    "\n",
    "**Exercise 2**\n",
    "\n",
    "Apply LSA to the weather or snowpack descriptions. Can you notice\n",
    "any pattern in the topics?\n",
    "\n",
    "([*back to text*](#exercise-1))\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "Apply LDA to the weather or snowpack descriptions. Can you notice\n",
    "any pattern in the topics?\n",
    "\n",
    "([*back to text*](#exercise-2))\n",
    "\n",
    "**Exercise 4**\n",
    "\n",
    "Use the reduced rank representation of text from LSA or LDA as a\n",
    "feature matrix to predict avalanche incidents. Compare the\n",
    "performance with the bag of words feature matrix.\n",
    "\n",
    "([*back to text*](#exercise-3))"
   ]
  }
 ],
 "metadata": {
  "filename": "working_with_text.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Working with Text"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}